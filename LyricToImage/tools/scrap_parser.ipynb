{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melon_parser():\n",
    "    # top 100 페이지 전처리\n",
    "\n",
    "    current_folder = Path.cwd() # 현재 폴더\n",
    "    parent_folder = current_folder.parent # 상위 폴더 이동\n",
    "    melon_folder = os.path.join(str(parent_folder) + '\\\\assets\\data\\stage1\\scrapper\\melon') # 크롤링한 Html 폴더 이동\n",
    "    Path(f'{melon_folder}\\\\result').mkdir(parents=True, exist_ok=True) # 결과 폴더 생성\n",
    "\n",
    "    ranking_data_html = os.listdir(melon_folder)[:-1] # result 폴더 제외한 html파일\n",
    "\n",
    "    for genre_rank_html in ranking_data_html: # 장르별 html파일 처리\n",
    "        if genre_rank_html.find(\".html\") != -1: # html 파일만 처리\n",
    "            \n",
    "            # csv 파일명 포매팅\n",
    "            file_name = re.search(r'\\d{4}-\\d{2}-\\d{2}', genre_rank_html).group() # 날짜부분 추출\n",
    "            genre_name = re.search(r'([a-zA-Z&]+)\\.html$', genre_rank_html).group(1) # 장르부분 추출\n",
    "            with open(f'{melon_folder}\\{genre_rank_html}', 'r') as file:\n",
    "                soup = bs(file, 'html.parser')\n",
    "                top100 = soup.find_all(\"tr\", {\"class\": \"lst50\"}) + soup.find_all(\"tr\", {\"class\": \"lst100\"}) # 100위까지 정보 합침\n",
    "                             \n",
    "                with open(f'{melon_folder}\\\\result\\{file_name}_{genre_name}.csv', 'w') as f:\n",
    "                    f.write(\"genre,rank,songid,songname,artistid,artistname,albumid,albumname\\n\")\n",
    "                    for row in top100:\n",
    "                        rank = row.find(\"span\", {\"class\": \"rank\"}).text.strip()\n",
    "                        songid = row.attrs['data-song-no']\n",
    "                        songname = f'\\\"{row.find(\"div\", {\"class\": \"ellipsis rank01\"}).text.strip()}\\\"'\n",
    "                        artistid = re.sub(r'[^0-9]', '', row.find(\"div\", {\"class\": \"ellipsis rank02\"}).find(\"a\").attrs['href'])\n",
    "                        artistname = row.find(\"div\", {\"class\": \"ellipsis rank02\"}).find(\"a\").text.strip()\n",
    "                        albumid = re.sub(r'[^0-9]', '', row.find(\"div\", {\"class\": \"ellipsis rank03\"}).find(\"a\").attrs['href'])\n",
    "                        albumname = f'\\\"{row.find(\"div\", {\"class\": \"ellipsis rank03\"}).text.strip()}\\\"'\n",
    "                        f.write(f'{genre_name},{rank},{songid},{songname},{artistid},{artistname},{albumid},{albumname}\\n')\n",
    "                  \n",
    "    # top 100에 등록된 곡 상세정보 전처리\n",
    "\n",
    "    top100_folder_path = os.path.join(melon_folder) + '\\\\top100' # c:\\Users\\Harvey\\Desktop\\Codes\\Python\\NLP\\LyricToImage\\assets\\data\\stage1\\scrapper\\melon\\top100\n",
    "    top100_folders = os.listdir(top100_folder_path) # ['ballad', 'dance', 'ost', 'pop', 'rap&hiphop', 'trot']\n",
    "\n",
    "\n",
    "    for genre_folder in top100_folders: # 장르별 처리\n",
    "        song_data_htmls = os.listdir(os.path.join(top100_folder_path) + f\"\\{genre_folder}\") # Html 파일 검색\n",
    "\n",
    "        with open(f'{melon_folder}\\\\result\\{file_name}_{genre_folder}_songdata.csv', 'w') as f:\n",
    "            # f.write(\"genre,songid,songname,artistid,artistname,albumid,albumname,release,flac,lyric,lyricist,composer,arranger\\n\")\n",
    "            f.write(\"genre,songid,songname,artistid,artistname,albumid,albumname,release,flac,lyric\\n\")\n",
    "            for song_data_html in song_data_htmls: # Html 파일별 처리\n",
    "                song_data_path = os.path.join(top100_folder_path + f'\\{genre_folder}\\{song_data_html}')\n",
    "\n",
    "                # Html 문서 읽기\n",
    "                with open(song_data_path, 'r') as file:\n",
    "                    d_soup = bs(file, 'html.parser')\n",
    "                    # 음악 id\n",
    "                    d_songid = d_soup.find(\"button\", {\"class\": \"button_etc like type02\"}).attrs['data-song-no'] \n",
    "                    \n",
    "                    # 제목\n",
    "                    d_songname_tag = d_soup.find(\"div\", {\"class\": \"song_name\"})\n",
    "                    strong_tag = d_songname_tag.find(\"strong\")\n",
    "                    strong_tag.decompose()\n",
    "                    d_songname = f'\\\"{d_songname_tag.text.strip()}\\\"'\n",
    "        \n",
    "                    # 아티스트 id, 이름\n",
    "                    d_artistid_n_name = d_soup.find(\"a\", {\"class\": \"artist_name\"}).attrs \n",
    "                    d_artistid = re.sub(r'[^0-9]', '', d_artistid_n_name['href']) \n",
    "                    d_artistname = f'\\\"{d_artistid_n_name[\"title\"]}\\\"'\n",
    "                    \n",
    "                    # 앨범 id, 앨범 제목, 발매일, 장르, FLAC 정보\n",
    "                    metadata = d_soup.find(\"dl\", {\"class\": \"list\"}).find_all(\"dd\")\n",
    "                    datum = []\n",
    "                    for idx, data in enumerate(metadata):\n",
    "                        if idx == 0:\n",
    "                            a_id = data.find(\"a\").attrs['href']\n",
    "                            a_id = re.sub(r'[^0-9]', '', a_id)\n",
    "                            datum.append(a_id)\n",
    "                        datum.append(data.text.strip())    \n",
    "                    d_albumid, d_albumname, d_releasedate, d_genre  = datum[0], f'\\\"{datum[1]}\\\"', datum[2], f'\\\"{datum[3]}\\\"'\n",
    "                    if len(datum) == 5:\n",
    "                        d_flactype = datum[4]\n",
    "                    else:\n",
    "                        d_flactype = \"\"\n",
    "                    # print(*datum)\n",
    "                    \n",
    "                    # 가사 정보\n",
    "                    try:\n",
    "                        lyric = d_soup.find(\"div\", {\"class\": \"lyric\"}).text.replace(\"\\n\", \" \").strip()\n",
    "                        lyric = lyric.replace(\",\", \"\").strip()\n",
    "                        lyric = re.sub(r\"\\s+\", \" \", lyric)\n",
    "                    except:\n",
    "                        lyric = \"\"\n",
    "                    lyric = f'\\\"{lyric}\\\"'\n",
    "                    \n",
    "                    # # 작사가, 작곡가, 편곡가 정보\n",
    "                    # l_c_a_data = []\n",
    "                    # l_c_a_s = d_soup.find(\"ul\", {\"class\": \"list_person clfix\"}).find_all(\"li\")\n",
    "                    # for l_c_a in l_c_a_s:\n",
    "                    #     l_c_a_data.append(l_c_a.find(\"div\", {\"class\": \"ellipsis artist\"}).text.strip())\n",
    "                    \n",
    "                    # d_lyricist, d_composer = l_c_a_data[0], l_c_a_data[1]\n",
    "                    # if len(l_c_a_data) == 3:\n",
    "                    #     d_arranger = l_c_a_data[2]\n",
    "                    # else:\n",
    "                    #     d_arranger = \"\"\n",
    "                        \n",
    "                f.write(f\"{d_genre},{d_songid},{d_songname},{d_artistid},{d_artistname},{d_albumid},{d_albumname},{d_releasedate},{d_flactype},{lyric}\\n\")\n",
    "                # f.write(f\"{d_genre},{d_songid},{d_songname},{d_artistid},{d_artistname},{d_albumid},{d_albumname},{d_releasedate},{d_flactype},{lyric},{d_lyricist},{d_composer},{d_arranger}\\n\")\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genie_parser():\n",
    "    current_folder = Path.cwd() # 현재 폴더\n",
    "    parent_folder = current_folder.parent # 상위 폴더 이동\n",
    "    genie_folder = os.path.join(str(parent_folder) + '\\\\assets\\data\\stage1\\scrapper\\genie') # 크롤링한 Html 폴더 이동\n",
    "    Path(f'{genie_folder}\\\\result').mkdir(parents=True, exist_ok=True) # 결과 폴더 생성\n",
    "\n",
    "    ranking_data_html_origin = os.listdir(genie_folder) # result 폴더 제외한 html파일\n",
    "    ranking_data_html = []\n",
    "    for i in range(0, len(ranking_data_html_origin), 2):\n",
    "        ranking_data_html.append(ranking_data_html_origin[i:i+2])\n",
    "  \n",
    "    for genre_rank_htmls in ranking_data_html[:-1]: # 장르별 html파일 처리\n",
    "        # csv 파일명 포매팅\n",
    "        file_name = re.search(r'\\d{4}-\\d{2}-\\d{2}', genre_rank_htmls[0]).group() # 날짜부분 추출\n",
    "        genre_name = re.search(r'([a-zA-Z&]+)_page([0-9])\\.html$', genre_rank_htmls[0]).group(1) # 장르부분 추출\n",
    "        \n",
    "        with open(f'{genie_folder}\\\\result\\{file_name}_{genre_name}.csv', 'w') as f:\n",
    "            f.write(f'genre,rank,songid,songname,artistid,artistname,albumid,albumname\\n')\n",
    "            for genre_rank_html in genre_rank_htmls:\n",
    "                if genre_rank_html.find(\".html\") != -1: # html 파일만 처리                \n",
    "                    with open(f'{genie_folder}\\{genre_rank_html}', 'r') as file:\n",
    "                        soups = bs(file, 'html.parser').find_all(\"tr\", {\"class\": \"list\"})\n",
    "                        for soup in soups:\n",
    "                            rank_tag = soup.find(\"td\", {\"class\": \"number\"})\n",
    "                            # 내부 공백 제거\n",
    "                            span_tag = rank_tag.find(\"span\", {\"class\": \"rank\"})\n",
    "                            span_tag.decompose()\n",
    "                            rank = f'{rank_tag.text.strip()}'\n",
    "                            \n",
    "                            songid = soup.attrs['songid']\n",
    "                            songname = soup.find(\"a\", {\"class\": \"title ellipsis\"}).text.strip()\n",
    "                            # 19 금 글자 제거\n",
    "                            songname = re.sub(r\"\\s+\", \" \", songname).replace(\"19 금 \", \"\")\n",
    "                            artistid = re.sub(r'[^0-9]', '', soup.find(\"a\", {\"class\": \"artist ellipsis\"}).attrs['ontouchend'])\n",
    "                            artistname = soup.find(\"a\", {\"class\": \"artist ellipsis\"}).text.strip()\n",
    "                            albumid = re.sub(r'[^0-9]', '', soup.find(\"a\", {\"class\": \"albumtitle ellipsis\"}).attrs['ontouchend'])\n",
    "                            albumname = soup.find(\"a\", {\"class\": \"albumtitle ellipsis\"}).text.strip()\n",
    "                            \n",
    "                            # 음악 제목이나 앨범제목에 콤마가 들어간 경우 \"\"로 감싸서 하나의 문자열로 입력\n",
    "                            f.write(f\"{genre_name},{rank},{songid},\\\"{songname}\\\",{artistid},{artistname},{albumid},\\\"{albumname}\\\"\\n\")\n",
    "    \n",
    "    # top 100에 등록된 곡 상세정보 전처리\n",
    "\n",
    "    top100_folder_path = os.path.join(genie_folder) + '\\\\top100' # c:\\Users\\Harvey\\Desktop\\Codes\\Python\\NLP\\LyricToImage\\assets\\data\\stage1\\scrapper\\genie\\top100\n",
    "    top100_folders = os.listdir(top100_folder_path) # ['kpop', 'ost', 'pop', 'trot']\n",
    "\n",
    "    for genre_folder in top100_folders: # 장르별 처리\n",
    "        song_data_htmls = os.listdir(os.path.join(top100_folder_path) + f\"\\{genre_folder}\") # Html 파일 검색\n",
    "        with open(f'{genie_folder}\\\\result\\{file_name}_{genre_folder}_songdata.csv', 'w') as f:\n",
    "            # f.write(\"genre,songid,songname,artistid,artistname,albumid,albumname,release,flac,lyric,lyricist,composer,arranger\\n\")\n",
    "            f.write(\"genre,songid,songname,artistid,artistname,albumid,albumname,playtime,playcnt,listencnt,likecnt,lyric\\n\")\n",
    "            for song_data_html in song_data_htmls: # Html 파일별 처리\n",
    "                song_data_path = os.path.join(top100_folder_path + f'\\{genre_folder}\\{song_data_html}')\n",
    "                \n",
    "                # Html 문서 읽기\n",
    "                with open(song_data_path, 'r') as file:\n",
    "                    d_soup_org = bs(file, 'html.parser')\n",
    "                    d_soup = d_soup_org.find(\"div\", {\"class\": \"song-main-infos\"})\n",
    "                    d_songid = d_soup_org.find(\"p\", {\"class\": \"song-button-zone\"}).find_all(\"a\")[1].attrs['songid']\n",
    "                    d_songname = d_soup.find(\"h2\", {\"class\": \"name\"}).text.strip()\n",
    "                    d_songname = re.sub(r\"\\s+\", \" \", d_songname).replace(\"19금 \", \"\")\n",
    "                    info_data = d_soup.find(\"ul\", {\"class\": \"info-data\"}).find_all(\"span\", {\"class\": \"value\"})\n",
    "                    info_list = [] #아티스트, 앨범명, 장르, 재생시간, 작사가, 작곡가, 편곡자\n",
    "                    for idx, data in enumerate(info_data):\n",
    "                        if idx == 0:\n",
    "                            d_artistid = re.sub(r'[^0-9]', '', data.find('a').attrs['onclick'])\n",
    "                        elif idx == 1:\n",
    "                            d_albumid = re.sub(r'[^0-9]', '', data.find('a').attrs['onclick']) \n",
    "                        info_list.append(data.text.strip().replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "                    \n",
    "                    play_listen = d_soup.find(\"div\", {\"class\": \"total\"}).find_all(\"p\")\n",
    "                    playcnt = int(play_listen[0].text.replace(\",\", \"\").strip())\n",
    "                    listencnt = int(play_listen[1].text.replace(\",\", \"\").strip())\n",
    "                    likecnt = int(d_soup.find(\"em\", {\"id\": \"emLikeCount\"}).text.replace(\",\", \"\").strip())\n",
    "                    try:\n",
    "                        lyric = d_soup_org.find(\"pre\", {\"id\": \"pLyrics\"}).find(\"p\").text.replace(\"\\n\", \"\").replace(\",\", \"\").strip()\n",
    "                    except:\n",
    "                        lyric = \"성인 이용자만 볼 수 있는 가사입니다.\"\n",
    "                        \n",
    "                    f.write(f'\\\"{info_list[2]}\\\",{d_songid},\\\"{d_songname}\\\",{d_artistid},{info_list[0]},{d_albumid},\\\"{info_list[1]}\\\",{info_list[3]},{playcnt},{listencnt},{likecnt},\\\"{lyric}\\\"\\n')\n",
    "                    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bugs_parer():\n",
    "    # top 100 페이지 전처리\n",
    "\n",
    "    current_folder = Path.cwd() # 현재 폴더\n",
    "    parent_folder = current_folder.parent # 상위 폴더 이동\n",
    "    bugs_folder = os.path.join(str(parent_folder) + '\\\\assets\\data\\stage1\\scrapper\\\\bugs') # 크롤링한 Html 폴더 이동\n",
    "    Path(f'{bugs_folder}\\\\result').mkdir(parents=True, exist_ok=True) # 결과 폴더 생성\n",
    "\n",
    "    ranking_data_html = os.listdir(bugs_folder)[:-1] # result 폴더 제외한 html파일\n",
    "    for genre_rank_html in ranking_data_html: # 장르별 html파일 처리\n",
    "        if genre_rank_html.find(\".html\") != -1: # html 파일만 처리\n",
    "\n",
    "            # csv 파일명 포매팅\n",
    "            file_name = re.search(r'\\d{4}-\\d{2}-\\d{2}', genre_rank_html).group() # 날짜부분 추출\n",
    "            genre_name = re.search(r'([a-zA-Z&]+)\\.html$', genre_rank_html).group(1) # 장르부분 추출\n",
    "            with open(f'{bugs_folder}\\\\result\\{file_name}_{genre_name}.csv', 'w') as f:\n",
    "                f.write(\"genre,rank,songid,songname,artistid,artistname,albumid,albumname\\n\")\n",
    "                with open(f'{bugs_folder}\\{genre_rank_html}', 'r') as file:\n",
    "                    soup = bs(file, 'html.parser')\n",
    "                    top100 = soup.find(\"tbody\").find_all(\"tr\")\n",
    "                    \n",
    "                    for row in top100:\n",
    "                        rank = row.find(\"div\", {\"class\": \"ranking\"}).find(\"strong\").text.strip()\n",
    "                        songid = row.attrs['trackid']\n",
    "                        songname = row.find(\"th\", {\"scope\": \"row\"}).find(\"a\").attrs['title']\n",
    "                        artistid = row.attrs['artistid']\n",
    "                        artistname = row.find_all(\"td\", {\"class\": \"left\"})[0].find(\"a\").attrs['title']\n",
    "                        albumid = row.attrs['albumid']\n",
    "                        albumname = row.find_all(\"td\", {\"class\": \"left\"})[1].find(\"a\").attrs['title']\n",
    "                        f.write(f'{genre_name},{rank},{songid},{songname},{artistid},{artistname},{albumid},{albumname}\\n')\n",
    "\n",
    "    # top 100에 등록된 곡 상세정보 전처리\n",
    "\n",
    "    top100_folder_path = os.path.join(bugs_folder) + '\\\\top100' \n",
    "    top100_folders = os.listdir(top100_folder_path) # ['ballad', 'dance', 'ost', 'pop', 'rap&hiphop', 'trot']                  \n",
    "                                \n",
    "    for genre_folder in top100_folders: # 장르별 처리\n",
    "        song_data_htmls = os.listdir(os.path.join(top100_folder_path) + f\"\\{genre_folder}\") # Html 파일 검색\n",
    "        \n",
    "        with open(f'{bugs_folder}\\\\result\\{file_name}_{genre_folder}_songdata.csv', 'w') as f:\n",
    "            f.write(\"genre,songid,songname,artistid,artistname,albumid,albumname,playtime,likecnt,flac,lyric\\n\")\n",
    "            for song_data_html in song_data_htmls:\n",
    "                song_data_path = os.path.join(top100_folder_path + f'\\{genre_folder}\\{song_data_html}')\n",
    "                with open(song_data_path, 'r') as file:\n",
    "                    soup = bs(file, 'html.parser')\n",
    "                    songid = soup.find(\"section\", {\"class\": \"commentsCommon sectionPadding\"}).attrs['cmt_target_id']\n",
    "                    songname = soup.find(\"article\").find(\"header\").find(\"h1\").text.strip()\n",
    "                    artistid = re.search(r'/artist/(\\d+)\\?', soup.find(\"tbody\").find(\"td\").find(\"a\").attrs[\"href\"]).group(1)\n",
    "                    albumid = re.search(r'/album/(\\d+)\\?', soup.find(\"li\", {\"class\": \"big\"}).find(\"a\").attrs[\"href\"]).group(1)\n",
    "                    meta_data = soup.find(\"tbody\").find_all(\"td\")\n",
    "                    \n",
    "                    \n",
    "                    # print(artistname, songname)              \n",
    "                    if len(meta_data) == 3:\n",
    "                        artistname = re.sub(r\"\\s+\", \" \", meta_data[0].text.replace(\"\\n\", \"\").replace(\"CONNECT 아티스트\", \"\").strip())\n",
    "                        albumname = re.sub(r\"\\s+\", \" \", meta_data[2].text.replace(\",\", \"\").strip())\n",
    "                        playtime = re.sub(r\"\\s+\", \" \", meta_data[-1].text.replace(\"\\n\", \"\").strip())\n",
    "                    elif len(meta_data) == 4:\n",
    "                        artistname = re.sub(r\"\\s+\", \" \", meta_data[0].text.replace(\"\\n\", \"\").replace(\"CONNECT 아티스트\", \"\").strip())\n",
    "                        if \"작곡\" in meta_data[1].text or \"보컬\" in meta_data[1].text: # 참여 정보가 있는 경우\n",
    "                            albumname = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-2].text.strip())\n",
    "                            playtime = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-1].text.replace(\"\\n\", \"\").strip())\n",
    "                        else:\n",
    "                            albumname = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-3].text.strip())\n",
    "                            playtime = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-2].text.replace(\"\\n\", \"\").strip())\n",
    "                    else:\n",
    "                        artistname = re.sub(r\"\\s+\", \" \", meta_data[0].text.replace(\"\\n\", \"\").replace(\"CONNECT 아티스트\", \"\").strip())\n",
    "                        albumname = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-3].text.strip())\n",
    "                        playtime = re.sub(r\"\\s+\", \" \", meta_data[len(meta_data)-2].text.replace(\"\\n\", \"\").strip())\n",
    "                        \n",
    "                        \n",
    "                    try:\n",
    "                        flac = re.search(r\"FLAC 16bit(?:, 24bit)?\", meta_data[-1].text.replace(\"\\n\", \"\").strip()).group()\n",
    "                    except:\n",
    "                        flac = \"\"\n",
    "                    try:\n",
    "                        lyric = re.sub(r\"\\s+\", \" \", soup.find(\"xmp\").text.replace(\"\\n\", \" \").replace(\",\", \" \").strip())\n",
    "                    except:\n",
    "                        lyric = \"\"\n",
    "                    likecnt = int(soup.find(\"em\").text.replace(\",\", \"\").strip())\n",
    "                    f.write(f'{genre_folder},{songid},\\\"{songname}\\\",{artistid},\\\"{artistname}\\\",{albumid},\\\"{albumname}\\\",{playtime},{likecnt},\\\"{flac}\\\",\\\"{lyric}\\\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "melon_parser()\n",
    "genie_parser()\n",
    "bugs_parer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
